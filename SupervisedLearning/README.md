[TOC]

# 监督学习

监督学习指的是有**目标变量或者预测目标**的机器学习方法，目标变量的值是已知的。
回归与分类的不同，就在于分类的目标变量是标称型数据，回归的目标变量是连续数值型

## 分类
分类问题属于监督学习，其目标变量是标称型数据。

KNN，决策树分类算法，数据实例最终会被**明确的**划分到某个分类中，属于**判别模型**。



### 分类算法

#### k-近邻算法

简单的实现k近邻分类，使用遍历的方法计算与训练集中每个样本的距离。如果使用kd树来存储数据可以减少计算量。
kNN算法首先有要足够的标记样本数据得到的效果才比较好。
KNN算法是基于实例的学习，使用算法是必须有接近实际数据的训练样本数据。
无法给出任何数据的基础结构信息，因此无法知晓实例样本和典型实例样本具有什么特征。
- 优点
精度高，对异常值不敏感，无数据输入假定
- 缺点
计算复杂度高、空间复杂度高
- 适用数据范围
数值型和标称型
##### example
手写系统识别

#### 决策树

决策树的主要优势在于数据形式非常容易理解，决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，根据数据集创建规则的过程，就是机器学习的过程。代码中使用的是ID3算法，选择信息增益最大的特征作为最优特征，没有进行剪枝操作，会overfitting，而且ID3算法无法直接处理 数值型 数据，而且会倾向与选择取值较多的特征。使用Python的字典类型存储树节点信息。
    

- 优点
计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
- 缺点
可能会产生过度匹配问题
- 适用数据类型
数值型和标称型

##### example

预测隐形眼镜类型

#### 朴素贝叶斯

’朴素‘，是因为整个形式化过程只做最原始、最简单的假设。准备数据：词袋模型（bag-of-words model）

- 优点

  在**数据较少**的情况下仍然有效，可以处理多类别问题。

- 缺点

  对于输入数据的准备方式较为敏感

- 适用数据类型

  标称型数据

##### example

过滤网站的恶意留言
过滤垃圾邮件
从个人广告中获取区域倾向



#### Logistic 回归

最优化算法，训练得到一个非线性函数用于分类。

回归：假设现在有一些数据点，用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合的过程就称作回归。“回归”一词源于最佳拟合，表示要找到最佳拟合参数集。

**Logistic回归进行分类的主要思想**：根据现有数据对**分类边界线**建立**回归公式**，以此进行分类。

Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。梯度上升，随机梯度上升，批量梯度上升。

- 优点

  计算代价不高，易于理解和实现

- 缺点

  容易欠拟合，分类精度可能不高

- 适用数据类型

  数值型和标称型数据

##### example

从疝气病症预测病马的死亡率

#### 支持向量机

数据线性可分，分隔超平面，间隔，支持向量

SVM 优化中一个特别好的地方就是，所有的运算都可以写成内积（看代码K[i,j]的意义之后理解）

核函数K(x,y)可以拆成 将输入空间（低维）映射到 特征空间（高维）,即由 x -> fai(x)
    但是，由于SMO算法中求解参数⍺时，用的时K(x,y)的值，而不需要拆开，因此不用考虑映射之后的向量是什么。
    常用的K(x,y)是：高斯径向核函数 Radial basis function kernel（RBF核）

对于最终用于分类的决策函数中，只需要支持向量来参与计算。

**核方法**或者说核技巧会将数据（有时是非线性数据）从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换成高维空间下的线性问题来求解。

支持向量的数目存在一个最优值。
    如果支持向量太少，就可能得到一个很差的决策边界
    如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法成为泡影 K近邻 。

这里的SVM是一个二分类器，有很多是基于SVM构建多分类器的。



- 优点

  泛化错误率低，计算开销不大，结果易解释

- 缺点

  对参数调节和核函数的选择敏感，原始分类器不加修改进适用于处理二类问题

- 使用数据类型

  数值型和标称型数据



##### example

手写识别问题（二分类，只有数字1和9）

#### 利用AdaBoost算法提高分类性能

通过组合相似的分类器来提高分类性能

训练过程中：会重新调整每个样本的权重，AdaBoost为每个分类器都分配了一个权重值





- 优点

  泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整

- 缺点

  对离群点敏感

- 适用数据类型

  数值型和标称型数据

##### example

从疝气病症预测病马的死亡率

### 处理非均衡问题
- 分类代价

  大多数情况下，不同类别的分类代价并不相等。

- 其他分类性能度量指标

  正确率、召回率和ROC曲线

  



## 回归

回归属于监督学习，其目标变量是**连续数值型**。预测数值型数据

### 回归算法

#### 线性回归



#### 局部加权线性回归



#### 岭回归和逐步线性回归



#### 树回归



